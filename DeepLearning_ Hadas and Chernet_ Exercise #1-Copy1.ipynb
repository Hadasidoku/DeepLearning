{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23615f4",
   "metadata": {},
   "source": [
    "# Deep Learning Exercise #1 _ Hadas Halperin: 315679225 & Chernet Maru: 327441440\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c7a8a",
   "metadata": {},
   "source": [
    "### Installing packages, list installed packages, upgrade package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bc9becc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "326a7737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828f694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hadasidoku\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce70ac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "ERROR: No matching distribution found for os\n"
     ]
    }
   ],
   "source": [
    "!pip install os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abfcef",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e07c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bf9a41",
   "metadata": {},
   "source": [
    "### Extracting Files From Folders and Number Length Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d340c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train folder we have 1341 normal pictures and Pneumonia: 3875 pictures\n",
      "In test folder we have  234 pictures, and as for Pneumonia: 390 pictures\n",
      "So, in train folder we have in total:  5216\n",
      "and in test folder we have in total: 624\n"
     ]
    }
   ],
   "source": [
    "Read_from_path = r'C:\\Users\\Hadasidoku\\Desktop\\deep learning project\\chest_xray\\chest_xray' #Files are instored internally\n",
    "\n",
    "Read_Train_Folder = os.path.join(Read_from_path, 'train')\n",
    "Read_Validation_Folder = os.path.join(Read_from_path, 'val')\n",
    "Read_Test_Folder = os.path.join(Read_from_path, 'test')\n",
    "\n",
    "#Extract the path of Positive and Negative Folders of Train and Test (we want to devide later the train to validation&train)\n",
    "Read_Pos_Train_Folder = os.path.join(Read_Train_Folder, 'PNEUMONIA')\n",
    "Read_Neg_Train_folder = os.path.join(Read_Train_Folder, 'NORMAL')\n",
    "Read_Pos_Test_folder = os.path.join(Read_Test_Folder, 'PNEUMONIA')\n",
    "Read_Neg_Test_folder = os.path.join(Read_Test_Folder, 'NORMAL')\n",
    "\n",
    "# Extracting file's names from path \n",
    "Train_Pneumonia_Files = os.listdir(Read_Pos_Train_Folder)\n",
    "Train_Normal_Files = os.listdir(Read_Neg_Train_folder)\n",
    "Test_Pneumonia_Files = os.listdir(Read_Pos_Test_folder)\n",
    "Test_Normal_Files = os.listdir(Read_Neg_Test_folder)\n",
    "\n",
    "# How many files do we have? \n",
    "print (\"In train folder we have\",len(Train_Normal_Files),\"normal pictures\", \"and Pneumonia:\" , len(Train_Pneumonia_Files),\"pictures\")\n",
    "print (\"In test folder we have \",len(Test_Normal_Files), \"pictures, and as for Pneumonia:\" , len(Test_Pneumonia_Files),\"pictures\")\n",
    "print(\"So, in train folder we have in total: \", len(Train_Pneumonia_Files+Train_Normal_Files))\n",
    "print(\"and in test folder we have in total:\", len(Test_Pneumonia_Files+Test_Normal_Files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d6188",
   "metadata": {},
   "source": [
    "### Section number A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5efad043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n",
      "Using 4695 files for training.\n",
      "Found 5216 files belonging to 2 classes.\n",
      "Using 521 files for validation.\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Image shape is:  (224, 224, 1)\n",
      "Train batch size is 261\n",
      "Test batch size is 32\n",
      "Classes list:  {'NORMAL': 0, 'PNEUMONIA': 1}\n",
      "Number of classes model is :  2\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=20\n",
    "\n",
    "# Normalization\n",
    "\n",
    "Train_Normalize = ImageDataGenerator(rescale=1./255)\n",
    "Test_Normalize = ImageDataGenerator(rescale=1./255)\n",
    "Val_Normalize = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Redefine names\n",
    "train_directory=Read_Train_Folder\n",
    "test_directory=Read_Test_Folder\n",
    "val_directory=Read_Validation_Folder\n",
    "\n",
    "\n",
    "\n",
    "# Deviding the training folder to train & validation\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_directory, seed=3, validation_split=0.1, subset='training')\n",
    "    \n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_directory, seed=3, validation_split=0.1, subset='validation')\n",
    "\n",
    "# resizing and dividing to batches\n",
    "Train_Data=Train_Normalize.flow_from_directory(train_directory, target_size=(224,224),class_mode='binary', batch_size= BATCH_SIZE, color_mode='grayscale')\n",
    "Test_Data=Test_Normalize.flow_from_directory(test_directory, target_size=(224,224),class_mode='binary',batch_size= BATCH_SIZE, color_mode='grayscale')\n",
    "Val_Data=Val_Normalize.flow_from_directory(val_directory, target_size=(224,224),class_mode='binary', batch_size= 2, color_mode='grayscale')\n",
    "\n",
    "# Validate the shape and batch size\n",
    "print('Image shape is: ',Test_Data.image_shape)\n",
    "print('Train batch size is',len(Train_Data))\n",
    "print('Test batch size is',len(Test_Data))\n",
    "print(\"Classes list: \",Train_Data.class_indices)\n",
    "print(\"Number of classes model is : \",Train_Data.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7c612",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f160ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " Dense1 (Dense)              (None, 64)                3211328   \n",
      "                                                                 \n",
      " Dense2 (Dense)              (None, 64)                4160      \n",
      "                                                                 \n",
      " Dense3 (Dense)              (None, 64)                4160      \n",
      "                                                                 \n",
      " Dense4 (Dense)              (None, 32)                2080      \n",
      "                                                                 \n",
      " Dense5 (Dense)              (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,221,761\n",
      "Trainable params: 3,221,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(name=\"my_sequential\")\n",
    "model.add(layers.Flatten(input_shape=(224,224,1)))\n",
    "model.add(layers.Dense(64, activation='relu', name='Dense1'))\n",
    "model.add(layers.Dense(64, activation='relu', name='Dense2'))\n",
    "model.add(layers.Dense(64, activation='relu', name='Dense3'))\n",
    "model.add(layers.Dense(32, activation='elu', name='Dense4'))\n",
    "model.add(layers.Dense(1, activation='sigmoid', name='Dense5'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cff7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer = Adam(learning_rate=1e-6), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c115c",
   "metadata": {},
   "source": [
    "### Section number B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff0245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "261/261 [==============================] - 125s 472ms/step - loss: 0.5161 - accuracy: 0.7488 - val_loss: 0.7505 - val_accuracy: 0.5000\n",
      "Epoch 2/70\n",
      "261/261 [==============================] - 97s 372ms/step - loss: 0.4033 - accuracy: 0.8282 - val_loss: 0.6359 - val_accuracy: 0.6250\n",
      "Epoch 3/70\n",
      "261/261 [==============================] - 98s 374ms/step - loss: 0.3464 - accuracy: 0.8763 - val_loss: 0.5999 - val_accuracy: 0.6875\n",
      "Epoch 4/70\n",
      "261/261 [==============================] - 97s 373ms/step - loss: 0.3095 - accuracy: 0.8949 - val_loss: 0.6049 - val_accuracy: 0.6875\n",
      "Epoch 5/70\n",
      "261/261 [==============================] - 96s 368ms/step - loss: 0.2830 - accuracy: 0.9049 - val_loss: 0.5981 - val_accuracy: 0.6875\n",
      "Epoch 6/70\n",
      "261/261 [==============================] - 97s 372ms/step - loss: 0.2639 - accuracy: 0.9105 - val_loss: 0.5281 - val_accuracy: 0.8125\n",
      "Epoch 7/70\n",
      "261/261 [==============================] - 97s 371ms/step - loss: 0.2472 - accuracy: 0.9193 - val_loss: 0.5416 - val_accuracy: 0.8125\n",
      "Epoch 8/70\n",
      "261/261 [==============================] - 97s 372ms/step - loss: 0.2332 - accuracy: 0.9245 - val_loss: 0.4967 - val_accuracy: 0.8125\n",
      "Epoch 9/70\n",
      "261/261 [==============================] - 98s 377ms/step - loss: 0.2228 - accuracy: 0.9279 - val_loss: 0.4940 - val_accuracy: 0.8125\n",
      "Epoch 10/70\n",
      "261/261 [==============================] - 97s 373ms/step - loss: 0.2120 - accuracy: 0.9314 - val_loss: 0.4531 - val_accuracy: 0.8125\n",
      "Epoch 11/70\n",
      "261/261 [==============================] - 105s 404ms/step - loss: 0.2043 - accuracy: 0.9342 - val_loss: 0.4766 - val_accuracy: 0.8125\n",
      "Epoch 12/70\n",
      "261/261 [==============================] - 104s 400ms/step - loss: 0.1956 - accuracy: 0.9379 - val_loss: 0.4882 - val_accuracy: 0.8125\n",
      "Epoch 13/70\n",
      "261/261 [==============================] - 104s 398ms/step - loss: 0.1894 - accuracy: 0.9394 - val_loss: 0.4604 - val_accuracy: 0.8125\n",
      "Epoch 14/70\n",
      "261/261 [==============================] - 103s 396ms/step - loss: 0.1825 - accuracy: 0.9431 - val_loss: 0.4420 - val_accuracy: 0.8125\n",
      "Epoch 15/70\n",
      "261/261 [==============================] - 103s 393ms/step - loss: 0.1776 - accuracy: 0.9408 - val_loss: 0.4222 - val_accuracy: 0.8125\n",
      "Epoch 16/70\n",
      "261/261 [==============================] - 107s 412ms/step - loss: 0.1725 - accuracy: 0.9419 - val_loss: 0.4523 - val_accuracy: 0.8125\n",
      "Epoch 17/70\n",
      "261/261 [==============================] - 102s 392ms/step - loss: 0.1682 - accuracy: 0.9450 - val_loss: 0.4087 - val_accuracy: 0.8125\n",
      "Epoch 18/70\n",
      "261/261 [==============================] - 101s 386ms/step - loss: 0.1635 - accuracy: 0.9467 - val_loss: 0.4160 - val_accuracy: 0.8125\n",
      "Epoch 19/70\n",
      "261/261 [==============================] - 101s 386ms/step - loss: 0.1603 - accuracy: 0.9471 - val_loss: 0.4275 - val_accuracy: 0.8125\n",
      "Epoch 20/70\n",
      "261/261 [==============================] - 105s 403ms/step - loss: 0.1577 - accuracy: 0.9471 - val_loss: 0.3950 - val_accuracy: 0.8125\n",
      "Epoch 21/70\n",
      "261/261 [==============================] - 104s 397ms/step - loss: 0.1543 - accuracy: 0.9488 - val_loss: 0.4360 - val_accuracy: 0.8125\n",
      "Epoch 22/70\n",
      "261/261 [==============================] - 102s 389ms/step - loss: 0.1515 - accuracy: 0.9484 - val_loss: 0.3602 - val_accuracy: 0.8750\n",
      "Epoch 23/70\n",
      "261/261 [==============================] - 102s 389ms/step - loss: 0.1476 - accuracy: 0.9494 - val_loss: 0.3940 - val_accuracy: 0.8125\n",
      "Epoch 24/70\n",
      "261/261 [==============================] - 100s 384ms/step - loss: 0.1452 - accuracy: 0.9513 - val_loss: 0.4072 - val_accuracy: 0.8125\n",
      "Epoch 25/70\n",
      "261/261 [==============================] - 100s 384ms/step - loss: 0.1424 - accuracy: 0.9519 - val_loss: 0.3441 - val_accuracy: 0.8750\n",
      "Epoch 26/70\n",
      "261/261 [==============================] - 104s 400ms/step - loss: 0.1408 - accuracy: 0.9534 - val_loss: 0.3782 - val_accuracy: 0.8125\n",
      "Epoch 27/70\n",
      "261/261 [==============================] - 103s 395ms/step - loss: 0.1394 - accuracy: 0.9521 - val_loss: 0.3818 - val_accuracy: 0.8125\n",
      "Epoch 28/70\n",
      "261/261 [==============================] - 101s 387ms/step - loss: 0.1367 - accuracy: 0.9534 - val_loss: 0.3655 - val_accuracy: 0.8125\n",
      "Epoch 29/70\n",
      "261/261 [==============================] - 104s 397ms/step - loss: 0.1348 - accuracy: 0.9549 - val_loss: 0.3211 - val_accuracy: 0.8750\n",
      "Epoch 30/70\n",
      "261/261 [==============================] - 105s 402ms/step - loss: 0.1336 - accuracy: 0.9559 - val_loss: 0.3377 - val_accuracy: 0.8750\n",
      "Epoch 31/70\n",
      "206/261 [======================>.......] - ETA: 21s - loss: 0.1335 - accuracy: 0.9549"
     ]
    }
   ],
   "source": [
    "epochs = 70\n",
    "model_fit = model.fit(Train_Data,validation_data = Val_Data, epochs=epochs, batch_size=BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_fit.history.keys())\n",
    "plt.plot(model_fit.history['accuracy'])\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(model_fit.history['loss'])\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b010e",
   "metadata": {},
   "source": [
    "### Section number C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db552126",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_First_Model = model.predict(Test_Data) \n",
    "y_Prediction_First = Predictions_First_Model > 0.5\n",
    "y_True_First = Test_Data.classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ee5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "font = {\n",
    "    'family': 'Times New Roman',\n",
    "    'size': 12\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "mat = confusion_matrix(y_True_First, y_Prediction_First)\n",
    "sns.heatmap(mat, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall = TP / (TP + FN)\n",
    "#precision = TP /(TP + FP)\n",
    "\n",
    "recall = sklearn.metrics.recall_score(y_True_First, y_Prediction_First, pos_label=1)\n",
    "print(\"Recall prob of model is:\" ,recall)\n",
    "\n",
    "precision = sklearn.metrics.precision_score(y_True_First, y_Prediction_First, pos_label=1)\n",
    "print(\"Precision prob of the model is:\" ,precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437edfc",
   "metadata": {},
   "source": [
    "### Section number D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436c087",
   "metadata": {},
   "source": [
    "#### Defining a convolution model  - (model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281015e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "input_shape = (224, 224, 1)\n",
    "epochs = 70\n",
    "\n",
    "SecondMod = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding = \"same\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\" ,padding = \"same\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation=\"sigmoid\"), #sigmoid is ideal for classification problems\n",
    "    ]\n",
    ")\n",
    "\n",
    "SecondMod.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff216d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use binary labels thus we need loss with binarycrosstropy\n",
    "    \n",
    "SecondMod.compile(loss='binary_crossentropy', optimizer = Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "\n",
    "SecondMod_Fit = SecondMod.fit(Train_Data,validation_data = Val_Data, epochs = epochs ,batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SecondMod_Fit.history.keys())\n",
    "plt.plot(SecondMod_Fit.history['accuracy'])\n",
    "plt.plot(SecondMod_Fit.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "\n",
    "plt.plot(SecondMod_Fit.history['loss'])\n",
    "plt.plot(SecondMod_Fit.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SecondMod_Prediction = SecondMod.predict(Test_Data) \n",
    "y_Prediction_Second = SecondMod_Prediction > 0.5\n",
    "y_True_Second = Test_Data.classes \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "font = {\n",
    "    'family': 'Times New Roman',\n",
    "    'size': 12\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "mat = confusion_matrix(y_True_Second, y_Prediction_Second)\n",
    "sns.heatmap(mat, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "recall = sklearn.metrics.recall_score(y_True_Second, y_Prediction_Second, pos_label=1)\n",
    "print(\" The recall of model is:\" ,recall)\n",
    "\n",
    "precision = sklearn.metrics.precision_score(y_True_Second, y_Prediction_Second, pos_label=1)\n",
    "print(\"The precision of the model is:\" ,precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b1040",
   "metadata": {},
   "source": [
    "### Section number D - section B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05633965",
   "metadata": {},
   "source": [
    "### Define Data Augmantation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ab6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define image size and rescaling\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "  layers.Rescaling(1./255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
    "                                                 input_shape=(224, \n",
    "                                                              224,\n",
    "                                                              1)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2), #make our dataset bigger by choosing different random rotation\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2), # same with random zooms\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "epochs = 70\n",
    "\n",
    "Third_Model_CNN_Aug = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'), # define the layers with augmentation\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.4),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "Third_Model_CNN_Aug.compile(optimizer = Adam(learning_rate=1e-4),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "Third_Model_CNN_Aug_Fit=Third_Model_CNN_Aug.fit(Train_Data,validation_data = Val_Data, epochs=epochs, batch_size=BATCH_SIZE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f1bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Third_Model_CNN_Aug_Fit.history.keys())\n",
    "plt.plot(Third_Model_CNN_Aug_Fit.history['accuracy'])\n",
    "plt.plot(Third_Model_CNN_Aug_Fit.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "\n",
    "plt.plot(Third_Model_CNN_Aug_Fit.history['loss'])\n",
    "plt.plot(Third_Model_CNN_Aug_Fit.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c6d49",
   "metadata": {},
   "source": [
    "## Section E - changing the learning rate of Second Model from 1e-3 ==> 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658584c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "num_classes = 1\n",
    "input_shape = (224, 224, 1)\n",
    "epochs = 70\n",
    "\n",
    "SecondMod = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding = \"same\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\" ,padding = \"same\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation=\"sigmoid\"), #sigmoid is ideal for classification problems\n",
    "    ]\n",
    ")\n",
    "\n",
    "SecondMod.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SecondMod.compile(loss='binary_crossentropy', optimizer = Adam(learning_rate=1e-5), metrics=['accuracy'])\n",
    "\n",
    "SecondMod_Fit = SecondMod.fit(Train_Data,validation_data = Val_Data, epochs = epochs ,batch_size = BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SecondMod_Fit.history.keys())\n",
    "plt.plot(SecondMod_Fit.history['accuracy'])\n",
    "plt.plot(SecondMod_Fit.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "\n",
    "plt.plot(SecondMod_Fit.history['loss'])\n",
    "plt.plot(SecondMod_Fit.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SecondMod_Prediction = SecondMod.predict(Test_Data) \n",
    "y_Prediction_Second = SecondMod_Prediction > 0.5\n",
    "y_True_Second = Test_Data.classes \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "font = {\n",
    "    'family': 'Times New Roman',\n",
    "    'size': 12\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "mat = confusion_matrix(y_True_Second, y_Prediction_Second)\n",
    "sns.heatmap(mat, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
